{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d19fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: TC (https://tc25.github.io/)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_x = \"..\\Download_1\"\n",
    "\n",
    "# Folder to save the combined data\n",
    "folder_y = \"..\\Summaries\"\n",
    "\n",
    "# Read all CSV files in folder_x without an 'H' in the name\n",
    "csv_files = [file for file in os.listdir(folder_x) if file.endswith(\".csv\") and 'H' not in file]\n",
    "\n",
    "# List to store all unique 'Date/Time' values from individual datasets\n",
    "all_date_time_values = []\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for file in csv_files:\n",
    "    # Read the CSV file and start reading just before the \"Date/Time\" row appears in the header\n",
    "    date_time_row = None\n",
    "    with open(os.path.join(folder_x, file), 'r', encoding='unicode_escape') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if \"Date/Time\" in line:\n",
    "                date_time_row = i\n",
    "                break\n",
    "\n",
    "    # Check if \"Date/Time\" row is found, and if so, skip rows until that row\n",
    "    if date_time_row is not None:\n",
    "        data = pd.read_csv(\n",
    "            os.path.join(folder_x, file),\n",
    "            skiprows=range(0, date_time_row),  # Skip rows until \"Date/Time\" row\n",
    "            encoding='unicode_escape',\n",
    "        )\n",
    "\n",
    "        # Convert the 'Date/Time' column to datetime rounded to the nearest hour\n",
    "        data['Date/Time'] = pd.to_datetime(data['Date/Time']).dt.round('H')\n",
    "\n",
    "        # Append the unique 'Date/Time' values to the list\n",
    "        all_date_time_values.append(data['Date/Time'])\n",
    "\n",
    "# Concatenate all 'Date/Time' columns from individual datasets into a single Series\n",
    "all_date_time_values_series = pd.concat(all_date_time_values)\n",
    "\n",
    "# Remove duplicates from the 'all_date_time_values_series'\n",
    "unique_date_time_values = all_date_time_values_series.drop_duplicates()\n",
    "\n",
    "# Create a continuous 'Date/Time' range using the unique 'Date/Time' values from all datasets\n",
    "continuous_datetime_range = pd.date_range(start=min(unique_date_time_values), end=max(unique_date_time_values), freq='H')\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data_new = pd.DataFrame({'Date/Time': continuous_datetime_range})\n",
    "\n",
    "# Iterate over each CSV file again\n",
    "for file in csv_files:\n",
    "    # Read the CSV file and stop reading just before the \"Date/Time\" row appears in the header\n",
    "    date_time_row = None\n",
    "    with open(os.path.join(folder_x, file), 'r', encoding='unicode_escape') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if \"Date/Time\" in line:\n",
    "                date_time_row = i\n",
    "                break\n",
    "\n",
    "    # Check if \"Date/Time\" row is found, and if so, skip rows until that row\n",
    "    if date_time_row is not None:\n",
    "        data = pd.read_csv(\n",
    "            os.path.join(folder_x, file),\n",
    "            skiprows=range(0, date_time_row),  # Skip rows until \"Date/Time\" row\n",
    "            encoding='unicode_escape',\n",
    "        )\n",
    "\n",
    "        # Convert the 'Date/Time' column to datetime rounded to the nearest hour\n",
    "        data['Date/Time'] = pd.to_datetime(data['Date/Time']).dt.round('H')\n",
    "\n",
    "        # Rename the 'Value' column to the name of the file (excluding the \".csv\" extension)\n",
    "        value_column_name = os.path.splitext(file)[0]\n",
    "        data.rename(columns={'Value': value_column_name}, inplace=True)\n",
    "\n",
    "        # Drop the 'Unit' column if it exists\n",
    "        if 'Unit' in data.columns:\n",
    "            data.drop('Unit', axis=1, inplace=True)\n",
    "\n",
    "        # Merge data with the existing DataFrame, using 'Date/Time' as the key and an outer join\n",
    "        combined_data_new = pd.merge(\n",
    "            combined_data_new,\n",
    "            data,\n",
    "            on='Date/Time',\n",
    "            how='outer',\n",
    "        )\n",
    "\n",
    "# Save the combined data to folder_y\n",
    "combined_data_new.to_csv(os.path.join(folder_y, 'combined_data_1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c6274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
